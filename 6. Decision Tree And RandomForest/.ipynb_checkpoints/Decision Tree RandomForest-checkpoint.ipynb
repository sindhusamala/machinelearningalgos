{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"PEW8r5l7B6N0"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"XDcUAXjpGqVJ"},"source":["## Decision Tree\n","\n","Decision tree algorithm is one of the most versatile algorithms in machine learning which can perform both classification and regression analysis. It is very powerful and works great with complex datasets. Apart from that, it is very easy to understand and read. That makes it more popular to use. When coupled with ensemble techniques – which we will learn very soon- it performs even better.\n","As the name suggests, this algorithm works by dividing the whole dataset into a tree-like structure based on some rules and conditions and then gives prediction based on those conditions.\n","Let’s understand the approach to decision tree with a basic scenario. \n","Suppose it’s Friday night and you are not able to decide if you should go out or stay at home. Let the decision tree decide it for you.\n","\n","\n","<img src=\"Decision_tree1.PNG\" width=\"300\">\n","                         \n","Although we may or may not use the decision tree for such decisions, this was a basic example to help you understand how a decision tree makes a decision.\n","So how did it work?\n","*\tIt selects a root node based on a given condition, e.g. our root node was chosen as time >10 pm.\n","*\tThen, the root node was split into child notes based on the given condition. The right child node in the above figure fulfilled the condition, so no more questions were asked.\n","*\tThe left child node didn’t fulfil the condition, so again it was split based on a new condition.\n","*\tThis process continues till all the conditions are met or if you have predefined the depth of your tree, e.g. the depth of our tree is 3, and it reached there when all the conditions were exhausted.\n","\n","Let’s see how the parent nodes and condition is chosen for the splitting to work.\n","\n","#### Decision Tree for Regression\n","When performing regression with a decision tree, we try to divide the given values of X into distinct and non-overlapping regions, e.g. for a set of possible values X1, X2,..., Xp; we will try to divide them into J distinct and non-overlapping regions R1, R2, . . . , RJ.\n","For a given observation falling into the region Rj, the prediction is equal to the mean of the response(y) values for each training observations(x) in the region Rj. \n","The regions R1,R2, . . . , RJ  are selected in a way to reduce the following sum of squares of residuals :\n","\n","\n","<img src=\"formula1.PNG\" width=\"300\">\n","                                                        \n","Where, yrj (second term) is the mean of all the response variables in the region ‘j’.\n","\n","\n","\n","#### Recursive binary splitting(Greedy approach)\n","As mentioned above, we try to divide the X values into j regions, but it is very expensive in terms of computational time to try to fit every set of X values into j regions. Thus, decision tree opts for a top-down greedy approach in which nodes are divided into two regions based on the given condition, i.e. not every node will be split but the ones which satisfy the condition are split into two branches. It is called greedy because it does the best split at a given step at that point of time rather than looking for splitting a step for a better tree in upcoming steps. It decides a threshold value(say s) to divide the observations into different regions(j) such that the RSS for Xj>= s and Xj <s is minimum.\n","\n","\n","<img src=\"formula2.PNG\" width=\"400\">\n","                      \n","Here for the above equation, j and s are found such that this equation has the minimum value.\n","The regions R1, R2 are selected based on that value of s and j such that the equation above has the minimum value.\n","Similarly, more regions are split out of the regions created above based on some condition with the same logic. This continues until a stopping criterion (predefined) is achieved.\n","Once all the regions are split, the prediction is made based on the mean of observations in that region.\n","\n","The process mentioned above has a high chance of overfitting the training data as it will be very complex. \n","\n","\n","### Classification Trees\n","\n","Regression trees are used for quantitative data. In the case of qualitative data or categorical data, we use classification trees.  In regression trees, we split the nodes based on RSS criteria, but in classification, it is done using classification error rate, Gini impurity and entropy.\n","Let’s understand these terms in detail.\n","\n","#### Entropy\n","Entropy is the measure of randomness in the data. In other words, it gives the impurity present in the dataset.\n","\n","<img src=\"entropy.PNG\" width=\"300\">\n","                                           \n","When we split our nodes into two regions and put different observations in both the regions, the main goal is to reduce the entropy i.e. reduce the randomness in the region and divide our data cleanly than it was in the previous node. If splitting the node doesn’t lead into entropy reduction, we try to split based on a different condition, or we stop. \n","A region is clean (low entropy) when it contains data with the same labels and random if there is a mixture of labels present (high entropy).\n","Let’s suppose there are ‘m’ observations and we need to classify them into categories 1 and 2.\n","Let’s say that category 1 has ‘n’ observations and category 2 has ‘m-n’ observations.\n","\n","p= n/m  and    q = m-n/m = 1-p\n","\n","then, entropy for the given set is:\n","\n","\n","          E = -p*log2(p) – q*log2(q) \n","           \n","           \n","When all the observations belong to category 1, then p = 1 and all observations belong to category 2, then p =0, int both cases E =0, as there is no randomness in the categories.\n","If half of the observations are in category 1 and another half in category 2, then p =1/2 and q =1/2, and the entropy is maximum, E =1.\n","\n","\n","<img src=\"entropy1.PNG\" width=\"300\">\n","                                  \n","\n","#### Information Gain\n","Information gain calculates the decrease in entropy after splitting a node. It is the difference between entropies before and after the split. The more the information gain, the more entropy is removed. \n","\n","<img src=\"info_gain.PNG\" width=\"300\">\n","\n","                                 \n","Where, T is the parent node before split and X is the split node from T.\n","\n","A tree which is splitted on basis of entropy and information gain value looks like:\n","\n","<img src=\"entropy_tree.PNG\" width=\"900\">\n","\n","#### Ginni Impurity\n","According to wikipedia, ‘Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labelled if it was randomly labelled according to the distribution of labels in the subset.’\n","It is calculated by multiplying the probability that a given observation is classified into the correct class and sum of all the probabilities when that particular observation is classified into the wrong class.\n","Let’s suppose there are k number of classes and an observation belongs to the class ‘i’, then Ginni impurity is given as:\n","\n","<img src=\"ginni.PNG\" width=\"300\">\n","                                    \n","Ginni impurity value lies between 0 and 1, 0 being no impurity and 1 denoting random distribution.\n","The node for which the Ginni impurity is least is selected as the root node to split.\n","\n","\n","A tree which is splitted on basis of ginni impurity value looks like:\n","\n","<img src=\"tree_example.PNG\" width=\"900\">\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wGKGzVu0GqVS"},"source":["### Maths behind Decision Tree Classifier\n","Before we see the python implementation of decision tree. let's first understand the math behind the decision tree classfication. We will see how all the above mentioned terms are used for splitting.\n","\n","We will use a simple dataset which contains information about students of different classes and gender and see whether they stay in school's hostel or not."]},{"cell_type":"markdown","metadata":{"id":"kHVbisn7GqVT"},"source":["This is how our data set looks like :\n","\n","\n","<img src='data_class.PNG' width=\"200\">"]},{"cell_type":"markdown","metadata":{"id":"XEAo25FiGqVT"},"source":["Let's try and understand how the root node is selected by calcualting gini impurity. We will use the above mentioned data.\n","\n","We have two features which we can use for nodes: \"Class\" and \"Gender\".\n","We will calculate gini impurity for each of the features and then select that feature which has least gini impurity.\n","\n","Let's review the formula for calculating ginni impurity:\n","\n","<img src='example/gini.PNG' width=\"200\">\n","\n","Let's start with class, we will try to gini impurity for all different values in \"class\". \n","\n","<img src='example/1.PNG' width=\"500\">\n","\n","<img src='example/2.PNG' width=\"500\">\n","\n","<img src='example/3.1.PNG' width=\"500\">\n","\n","<img src='example/3.PNG' width=\"500\">\n","\n","<img src='example/4.PNG' width=\"500\">\n","\n","<img src='example/5.PNG' width=\"500\">\n","\n","<img src='example/6.PNG' width=\"500\">\n","\n","<img src='example/7.PNG' width=\"500\">\n","\n","<img src='example/8.PNG' width=\"500\">\n","\n","This is how our Decision tree node is selected by calculating gini impurity for each node individually.\n","If the number of feautures increases, then we just need to repeat the same steps after the selection of the root node."]},{"cell_type":"markdown","metadata":{"id":"IWlyaD_4GqVU"},"source":["We will try and find the root nodes for the same dataset by calculating entropy and information gain.\n","\n","DataSet:\n","\n","<img src='data_class.PNG' width=\"200\">\n","\n","We have two features and we will try to choose the root node by calculating the information gain by splitting each feature.\n","\n","Let' review the formula for entropy and information gain:\n","\n","<img src='example/formula_entropy.PNG' width=\"300\">\n","\n","<img src='example/inform_gain.PNG' width=\"300\">\n","\n","\n","Let's start with feature \"class\" :\n","\n","<img src='example/9.PNG' width=\"500\">\n","\n","<img src='example/10.1.PNG' width=\"500\">\n","\n","<img src='example/11.PNG' width=\"500\">\n","\n","<img src='example/12.PNG' width=\"500\">\n","\n","<img src='example/13.PNG' width=\"500\">\n","\n","\n","Let' see the information gain from feature \"gender\" :\n","\n","<img src='example/10.2.PNG' width=\"500\">\n","\n","<img src='example/14.PNG' width=\"500\">\n","\n","<img src='example/15.PNG' width=\"500\">\n","\n","<img src='example/16.PNG' width=\"500\">\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NGF42IqYGqVU"},"source":["### Different Algorithms for Decision Tree\n","\n","\n","* ID3 (Iterative Dichotomiser) : It is one of the algorithms used to construct decision tree for classification. It uses Information gain as the criteria for finding the root nodes and splitting them. It only accepts categorical attributes.\n","\n","* C4.5 : It is an extension of ID3 algorithm, and better than ID3 as it deals both continuous and discreet values.It is also used for classfication purposes.\n","\n","\n","* Classfication and Regression Algorithm(CART) : It is the most popular algorithm used for constructing decison trees. It uses ginni impurity as the default calculation for selecting root nodes, however one can use \"entropy\" for criteria as well. This algorithm works on both regression as well as classfication problems. We will use this algorithm in our pyhton implementation. \n","\n","\n","Entropy and Ginni impurity can be used reversibly. It doesn't affects the result much. Although, ginni is easier to compute than entropy, since entropy has a log term calculation. That's why CART algorithm uses ginni as the default algorithm.\n","\n","If we plot ginni vs entropy graph, we can see there is not much difference between them:\n","\n","<img src=\"example/entropyVsGini.PNG\" width = \"400\">\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vPxt4lUfGqVV"},"source":["##### Advantages of Decision Tree:\n","\n","   * It can be used for both Regression and Classification problems.\n","   * Decision Trees are very easy to grasp as the rules of splitting is clearly mentioned.\n","   * Complex decision tree models are very simple when visualized. It can be understood just by visualising.\n","   * Scaling and normalization are not needed.\n","\n","\n","##### Disadvantages of Decision Tree:\n","\n","\n","   * A small change in data can cause instability in the model because of the greedy approach.\n","   * Probability of overfitting is very high for Decision Trees.\n","   * It takes more time to train a decision tree model than other classification algorithms."]},{"cell_type":"markdown","metadata":{"id":"81MWsN13GqVV"},"source":["## Business Case:-Based on given features we need to find whether an employee will leave the company or not."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GTvp_TcrGqVW"},"outputs":[],"source":["## Importing the libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h9OlcUPLGqVW"},"outputs":[],"source":["## Target variable:-"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tt_FHAtHGqVX"},"outputs":[],"source":["## Loading the data\n","data=pd.read_csv('HR-Employee-Attrition.csv')"]},{"cell_type":"markdown","metadata":{"id":"SifTu1HxGqVZ"},"source":["## Basic Checks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pHCvnVhtGqVX","scrolled":true},"outputs":[],"source":["data.head()#first five rows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3hvXDP0MGqVY"},"outputs":[],"source":["## Getting all columns form the dataset\n","data.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XrfN1JniGqVY"},"outputs":[],"source":["data.head(pd.set_option('display.max_columns',None))#to diplay all columns from dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ChOdtVXGB6OL"},"outputs":[],"source":["data.HourlyRate.value_counts()#number of appearance for each label in the columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VaotZpJ3GqVZ"},"outputs":[],"source":["data.tail()#last five rows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DQJs1mkVGqVZ"},"outputs":[],"source":["data.describe()##used to view some basic statistical details like percentile, mean, std etc. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WzysLAguB6ON"},"outputs":[],"source":["# we do not have any null values\n","# we have one constant feature Employee count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nHxP8RSpGqVa"},"outputs":[],"source":["data.describe(include=['O'])#It will give you info about categorical data/columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2grCr8khGqVa"},"outputs":[],"source":["data.info()#To check  data type and  null value of all columns  "]},{"cell_type":"markdown","metadata":{"id":"lgxJSrcjGqVa"},"source":["## Exploratory Data Analysis"]},{"cell_type":"markdown","metadata":{"id":"ljxmgpUBB6OP"},"source":["# univariant Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ptW3IqPGqVa"},"outputs":[],"source":["## Univariate Analysis\n","!pip install sweetviz#installing sweetviz library`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYqXdgGvGqVb"},"outputs":[],"source":["import sweetviz as sv#importing sweetviz library \n","my_report = sv.analyze(data)#syntax to use sweetviz\n","my_report.show_html()#Default arguments will generate to \"SWEETVIZ_REPORT.html\""]},{"cell_type":"markdown","metadata":{"id":"MsdUs3hXB6OQ"},"source":["## insights from univariant\n","* people betwwen the age group 25-40 are the majority\n","* 70% of the people travel raraely, 20% travel frequently rest do not travel\n","* more than 70% of the employeres belong to research and development\n","* almost 50% of the people are nearer to the office i.e the distance from their home is lesser than or equal to 10.\n","* more than 60% of the people have educational qualification of 4 and 5\n","* majority(40%) of the people are from life science field and 30% are from medical field\n","* 60% of the people are almost satisfied with environment condition of the office with more than 3 ratings.\n","* gender count: 60% male 40% female\n","* 60% of the people have partial involvement in job and 20% have good involvement\n","* more than 60% employees seem to be satisfied with their job\n","* 50% of the people are married, 30% single and the rest are divorced\n","* 60% of the people have less thanm 10k income\n","* 40% of the people have worked for less than 1 company which implies they are freshers\n","* 30% of the people have worked for more than 5 companies\n","* 80% of the people have average work rating\n","* 60% of the people have worked for the same company only for 5 years or lesser\n","* 80% of the people own only 1 or 0 stock at the company"]},{"cell_type":"markdown","metadata":{"executionInfo":{"elapsed":6967,"status":"aborted","timestamp":1619775152050,"user":{"displayName":"DHANUSH Appala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkVBriqzTRKHlFTY43_1IwgVqGUQnUlVPkfsvVpRg=s64","userId":"17800607208793960461"},"user_tz":-330},"id":"t-ljpiZyGqVb"},"source":["# Bivaraite Analysis "]},{"cell_type":"markdown","metadata":{"id":"6qkSONNUB6OR"},"source":["## checking relationship of all variables with respect to target variable "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bhAo8-QaGqVc"},"outputs":[],"source":["categorical_col = []#list\n","for column in data.columns:#for loop to acess columns form dataset\n","    if data[column].dtype == object and len(data[column].unique()) <= 50:#checking datatype whether datatype is object/string and number of unique label in the columns less than 50 \n","        categorical_col.append(column)#appending those columns in the list who statisfy the condition \n","        print(f\"{column} : {data[column].unique()}\")#output\n","        print(\"====================================\")"]},{"cell_type":"markdown","metadata":{"executionInfo":{"elapsed":6955,"status":"aborted","timestamp":1619775152052,"user":{"displayName":"DHANUSH Appala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkVBriqzTRKHlFTY43_1IwgVqGUQnUlVPkfsvVpRg=s64","userId":"17800607208793960461"},"user_tz":-330},"id":"rQ_KEiy5GqVc"},"source":["## Categorical Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FX0KQNHSGqVc"},"outputs":[],"source":["## Create a new dataframe with categorical variables only\n","data1=data[['Attrition',\n"," 'BusinessTravel',\n"," 'Department',\n"," 'EducationField',\n"," 'Gender',\n"," 'JobRole',\n"," 'MaritalStatus',\n"," 'Over18',\n"," 'OverTime']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"97QP0vXTGqVc"},"outputs":[],"source":["data1#new data frame with categorical columns only"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K11mCBysGqVd"},"outputs":[],"source":["# Plotting how every  categorical feature correlate with the \"target\"\n","plt.figure(figsize=(50,50), facecolor='white')#canvas size\n","plotnumber = 1#count variable\n","\n","for column in data1:#for loop to acess columns form data1\n","    if plotnumber<=16 :#checking whether count variable is less than 16 or not\n","        ax = plt.subplot(4,4,plotnumber)#plotting 8 graphs in canvas(4 rows and 4 columns)\n","        sns.countplot(x=data1[column].dropna(axis=0)#plotting count plot \n","                        ,hue=data.Attrition)\n","        plt.xlabel(column,fontsize=20)#assigning name to x-axis and increasing it's font \n","        plt.ylabel('Attrition',fontsize=20)#assigning name to y-axis and increasing it's font \n","    plotnumber+=1#increasing counter\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"xgqNevRnB6OT"},"source":["## insights of bivariant\n","* these are the insights wrt attrition\n","* more male employees are expected to quit their job\n","* people who travel more are more expected to leave the job\n","* people who do not do overtime do not leave the job\n","* singles are expected to quit the job\n","* people from life science and mediacl field are more probablyu leaving theitr job"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qIPipKdxB6OT"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4aBbGbVoGqVd"},"outputs":[],"source":["numerical_col = []#list for continous columns\n","for column in data.columns:#acessing columns from datasets\n","    if data[column].dtype == int and len(data[column].unique()) >= 10: #checking whether it's datatype is int and count of unique label greater than 10  \n","        numerical_col.append(column) # inserting those columns in list                                      \n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tsyePR9fB6OU"},"outputs":[],"source":["numerical_col#printing list which contain continous columns"]},{"cell_type":"markdown","metadata":{"id":"xMnzw7AxGqVd"},"source":["##  Discrete data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NTWZl0nTGqVe"},"outputs":[],"source":["data3=data[['Education',\n"," 'EmployeeCount',\n"," 'EnvironmentSatisfaction',\n"," 'JobInvolvement',\n"," 'JobLevel',\n"," 'JobSatisfaction',\n"," 'NumCompaniesWorked',\n"," 'PerformanceRating',\n"," 'RelationshipSatisfaction',\n"," 'StandardHours',\n"," 'StockOptionLevel',\n"," 'TrainingTimesLastYear',\n"," 'WorkLifeBalance']]#discrete columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HNzZGjgTGqVe"},"outputs":[],"source":["# Plotting how every  discrete feature correlate with the \"target\"\n","plt.figure(figsize=(20,25), facecolor='white')#canvas size\n","plotnumber = 1\n","\n","for column in data3:\n","    if plotnumber<=16 :\n","        ax = plt.subplot(4,4,plotnumber)\n","        sns.countplot(x=data3[column].dropna(axis=0)\n","                        ,hue=data.Attrition)\n","        plt.xlabel(column,fontsize=20)\n","        plt.ylabel('Attrition',fontsize=20)\n","    plotnumber+=1\n","plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-2ymsT6fB6OV"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"PTwojgULB6OV"},"source":["### Bivariant analysis of continuous variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5DwOl-OhGqVf"},"outputs":[],"source":["data2=data[['Age',\n"," 'DailyRate',\n"," 'DistanceFromHome',\n"," 'EmployeeNumber',\n"," 'HourlyRate',\n"," 'MonthlyIncome',\n"," 'MonthlyRate',\n"," 'NumCompaniesWorked',\n"," 'PercentSalaryHike',\n"," 'TotalWorkingYears',\n"," 'YearsAtCompany',\n"," 'YearsInCurrentRole',\n"," 'YearsSinceLastPromotion',\n"," 'YearsWithCurrManager']]#continuous variables/columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R82tDyP2GqVf"},"outputs":[],"source":["# Plotting how every  numerical feature correlate with the \"target\"\n","plt.figure(figsize=(20,25), facecolor='white')#canvas size\n","plotnumber = 1#counter for number of plot\n","\n","for column in data2:#acessing columns form data2 DataFrame\n","    if plotnumber<=16 :#checking whether counter is less than 16 or not\n","        ax = plt.subplot(4,4,plotnumber)#plotting 8 graphs in canvas(4 rows and 4 columns)\n","        sns.histplot(x=data2[column].dropna(axis=0)# plotting hist plot and dropping null values,classification according to target\n","                        ,hue=data.Attrition)\n","        plt.xlabel(column,fontsize=20)##assigning name to x-axis and increasing it's font \n","        plt.ylabel('Attrition',fontsize=20)#assigning name to y-axis and increasing it's font \n","    plotnumber+=1#increasing counter by 1\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"NnQIsJzMGqVf"},"source":["## Final conclusions\n","BusinessTravel : The workers who travel alot are more likely to quit then other employees.\n","\n","Department : The worker in Research & Development are more likely to stay then the workers on other departement.\n","\n","EducationField : The workers with Human Resources and Technical Degree are more likely to quit then employees from other fields of educations.\n","\n","Gender : The Male are more likely to quit.\n","\n","JobRole : The workers in Laboratory Technician, Sales Representative, and Human Resources are more likely to quit the workers in other positions.\n","\n","MaritalStatus : The workers who have Single marital status are more likely to quit the Married, and Divorced.\n","\n","OverTime : Attrition rate is almost equal"]},{"cell_type":"markdown","metadata":{"id":"Q9k1w33zGqVg"},"source":["# Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"XQ8Moe5DB6OW"},"source":["## Checking missing values/null values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sa-TlTe8GqVg"},"outputs":[],"source":["\n","data.isnull().sum()#null value checking \n","# no null values"]},{"cell_type":"markdown","metadata":{"id":"OZPyFyfxB6OW"},"source":["# conversion of  Categorical columns in to numerical columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_J_u9FdIGqVg"},"outputs":[],"source":["## Categorical data conversion\n","data1.head()"]},{"cell_type":"markdown","metadata":{"id":"3vPUDiroB6OX"},"source":["###  1.Attrition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n2ddHOLsGqVh"},"outputs":[],"source":["data.Attrition.unique()#checking unique value in Attrition column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8jvPI4xGqVg","scrolled":true},"outputs":[],"source":["## Manual encoding Attrition feature\n","data.Attrition=data.Attrition.map({'Yes':1,'No':0})\n","data1.head()\n"]},{"cell_type":"markdown","metadata":{"id":"cAvuCVgAB6OX"},"source":["###  2.BusinessTravel "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJlm5UBuB6OY"},"outputs":[],"source":["data.BusinessTravel.unique()#checking unique value"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V2APHYAdGqVh"},"outputs":[],"source":["## Encoding BusinessTravel, this feature told the worker who travelled frequesnlty has quited the job so let do the\n","##manual encoding\n","data.BusinessTravel=data.BusinessTravel.map({'Travel_Frequently':2,'Travel_Rarely':1,'Non-Travel':0})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mVUQ5MzcGqVh"},"outputs":[],"source":["data.head()#checking whether imputation properly done or not "]},{"cell_type":"markdown","metadata":{"id":"EVI5suuHB6OZ"},"source":["### 3.Department"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"esJmaUIJB6Oa"},"outputs":[],"source":["data.Department.unique()#unique values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YHE92tgcGqVh"},"outputs":[],"source":["data.Department=data.Department.map({'Research & Development':2,'Sales':1,'Human Resources':0})#imputation using map function\n"]},{"cell_type":"markdown","metadata":{"id":"NPhY1PIyB6Ob"},"source":["### 4.EducationField"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fb7GhgliB6Ob"},"outputs":[],"source":[" data.EducationField.unique()#unique labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"450XCLRIGqVi"},"outputs":[],"source":["#using map function\n","data.EducationField=data.EducationField.map({'Life Sciences':5,'Medical':4,'Marketing':3,'Technical Degree':2,'Other':1,'Human Resources':0 })\n","   \n"," "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qLTareTGqVi"},"outputs":[],"source":["data.head()#checking for imputation"]},{"cell_type":"markdown","metadata":{"id":"qYdl1kNBB6Ob"},"source":["### 5.Gender"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APyj9DcRGqVi","scrolled":true},"outputs":[],"source":["data.Gender.value_counts()#checking weightage of each label whoever have high count "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VKyEmGfDGqVi"},"outputs":[],"source":["## Encoding Gender\n","data.Gender=pd.get_dummies(data.Gender,drop_first=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"upglP5buB6Oc"},"outputs":[],"source":["data.Gender#checking whether imputation done or not\n"]},{"cell_type":"markdown","metadata":{"id":"29QZOplUB6Oc"},"source":["### JobRole"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fkOT2a99B6Oc"},"outputs":[],"source":["data.JobRole.value_counts()#checking count for each label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9FVbyyqkGqVi"},"outputs":[],"source":["## Encoding JobRole\n","data.JobRole=data.JobRole.map({'Laboratory Technician':8,'Sales Executive':7,'Research Scientist':6,'Sales Representative':5,\n","                              'Human Resources':4,'Manufacturing Director':3,'Healthcare Representative':2,'Manager':1,'Research Director':0 })\n","  \n","   \n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ehJ6tv84GqVj"},"outputs":[],"source":["data.JobRole#data.Gender#checking whether imputation done or not\n"]},{"cell_type":"markdown","metadata":{"id":"V77KlVOaB6Od"},"source":["### Encoding MaritalStatus using label encoding \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Lkw25rOGqVj"},"outputs":[],"source":["## Encoding MaritalStatus\n","\n","from sklearn.preprocessing import LabelEncoder#importing label encoder from sklearn \n","\n","label = LabelEncoder()#object creation \n","data.MaritalStatus=label.fit_transform(data.MaritalStatus)#applying label encoder to  marital status"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bswiSdTYGqVj"},"outputs":[],"source":["data.MaritalStatus"]},{"cell_type":"markdown","metadata":{"id":"r5iURHvkB6Od"},"source":["### OverTime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9fhBprtnGqVj"},"outputs":[],"source":["## Encoding OverTime\n","data.OverTime=label.fit_transform(data.OverTime)#label encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xhSE2-g4GqVk"},"outputs":[],"source":["data.head()#checking for imputation "]},{"cell_type":"markdown","metadata":{"id":"eY7sHt4EGqVk"},"source":["## Feature Selection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eplBC4upGqVk"},"outputs":[],"source":["## Checking correlation\n","\n","plt.figure(figsize=(30, 30))#canvas size\n","sns.heatmap(data2.corr(), annot=True, cmap=\"RdYlGn\", annot_kws={\"size\":15})#plotting heat map to check correlation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0IZ7NazaGqVk"},"outputs":[],"source":["## Removing constant features\n","data.drop(['EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours'], axis=\"columns\", inplace=True)#droping those columns which have std=0 "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oduELA79B6Oe"},"outputs":[],"source":["data.describe()"]},{"cell_type":"markdown","metadata":{"id":"TNccjAhwGqVl"},"source":["## Model Creation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hstSuG5WGqVl"},"outputs":[],"source":["## Creating independent and dependent variable\n","X = data.drop('Attrition', axis=1)#independent variable \n","y = data.Attrition#dependent variable "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2-vfEK3qGqVl"},"outputs":[],"source":["## Balacing the data\n","from collections import Counter# importing counter to check count of each label\n","from imblearn.over_sampling import SMOTE #for balancing the data\n","sm=SMOTE()#object creation\n","print(Counter(y))# checking count for each class \n","X_sm,y_sm=sm.fit_resample(X,y)#applying sampling on target variable \n","print(Counter(y_sm))# checking count after sampling for  each class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ENFDdjN6GqVl"},"outputs":[],"source":["## preparing training and testing data\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.25, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bO4z0WfgGqVl"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier#importing decision tree from sklearn.tree\n","dt=DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_leaf= 1, min_samples_split= 3, splitter= 'random')#object creation for decision tree  \n","dt.fit(X_train,y_train)#training the model\n","y_hat=dt.predict(X_test)#prediction\n","y_hat#predicted values "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJhwmkchB6Of"},"outputs":[],"source":["y_train_predict=dt.predict(X_train)#predicting training data to check training performance \n","y_train_predict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oLebPUzFGqVm"},"outputs":[],"source":["## Evalauting the model\n","from sklearn.metrics import accuracy_score,classification_report,f1_score    #importing mertics to check model performance\n","##Training score\n","y_train_predict=dt.predict(X_train)#passing X_train to predict Y_train\n","acc_train=accuracy_score(y_train,y_train_predict)#checking accuracy\n","acc_train\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gcZKrmmXGqVm"},"outputs":[],"source":["print(classification_report(y_train,y_train_predict))# it will give precision,recall,f1 scores and accuracy  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_nWcvr9AGqVm"},"outputs":[],"source":["pd.crosstab(y_train,y_train_predict)#it will show you confusion matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SHIIwd0tGqVm","scrolled":true},"outputs":[],"source":["## test acc\n","test_acc=accuracy_score(y_test,y_hat)#testing accuracy \n","test_acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OH29Hq0GGqVn"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ndTtoD1dGqVn"},"outputs":[],"source":["## test score\n","test_f1=f1_score(y_test,y_hat)#f1 score\n","test_f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uzKcjjRBGqVn"},"outputs":[],"source":["print(classification_report(y_test,y_hat))# for  testing "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YGdOtp55GqVn"},"outputs":[],"source":["pd.crosstab(y_test,y_hat)# confusion matrix for"]},{"cell_type":"markdown","metadata":{"id":"xHp4KJoMGqVn"},"source":["## Hyperparameters of DecisionTree\n","* Hyperparameter tuning is searching the hyperparameter space for a set of values that will optimize your model architecture.\n"]},{"cell_type":"markdown","metadata":{"id":"3sXgNhQKGqVo"},"source":["* criterion: The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n","\n","\n","* splitter: This is how the decision tree searches the features for a split. The default value is set to “best”. That is, for each node, the algorithm considers all the features and chooses the best split. If you decide to set the splitter parameter to “random,” then a random subset of features will be considered.\n","\n","\n","\n","* max_depth: This determines the maximum depth of the tree.  we use a depth of two to make our decision tree. ... This will often result in over-fitted decision trees. The depth parameter is one of the ways in which we can regularize the tree, or limit the way it grows to prevent over-fitting..The tree perfectly fits the training data and fails to generalize on testing data.\n","\n","\n","\n","* min_samples_split:Ideal range is 1 to 40.min_samples_split specifies the minimum number of samples required to split an internal node, while min_samples_leaf specifies the minimum number of samples required to be at a leaf node.\n","\n","\n","\n","* min_samples_leaf: The minimum number of samples required to be at a leaf node.Similarr to min sample split ,this describes the minimum number of samples at the leaf,the base of tree.Ideal range is 1 to 20.(thershold value to make a decision)like 40\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8R_1JZNB6Oh"},"outputs":[],"source":["https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hhVF_1VgGqVo"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","#It helps to loop through predefined hyperparameters and fit your estimator (model) on your training set. \n","#So,in the end, you can select the best parameters from the listed hyperparameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CI6CDPDyGqVo","scrolled":true},"outputs":[],"source":["\n","#creating dictionary--> key value pair of hyperparameters having key as parameter and values as its values\n","params = {\n","    \"criterion\":(\"gini\", \"entropy\"), #quality of split\n","    \"splitter\":(\"best\", \"random\"), # searches the features for a split\n","    \"max_depth\":(list(range(1, 20))), #depth of tree range from 1 to 19\n","    \"min_samples_split\":[2, 3, 4],    #the minimum number of samples required to split internal node\n","    \"min_samples_leaf\":list(range(1, 20)),#minimum number of samples required to be at a leaf node,we are passing list which is range from 1 to 19 \n","}\n","\n","\n","tree_clf = DecisionTreeClassifier(random_state=3)#object creation for decision tree with random state 3\n","tree_cv = GridSearchCV(tree_clf, params, scoring=\"f1\", n_jobs=-1, verbose=1, cv=3)\n","#passing model to gridsearchCV ,\n","#tree_clf-->model\n","#params---->hyperparametes(dictionary we created)\n","#scoring--->performance matrix to check performance\n","#n_jobs---->Number of jobs to run in parallel,-1 means using all processors.\n","#verbose=Controls the verbosity: the higher, the more messages.\n","#>1 : the computation time for each fold and parameter candidate is displayed;\n","#>2 : the score is also displayed;\n","#>3 : the fold and candidate parameter indexes are also displayed together with the starting time of the computation.\n","#cv------> number of flods\n","\n","\n","\n","\n","tree_cv.fit(X_train,y_train)#training data on gridsearch cv\n","best_params = tree_cv.best_params_#it will give you best parameters \n","print(f\"Best paramters: {best_params})\")#printing  best parameters\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nuruanjWB6Oi"},"outputs":[],"source":["#fitting 3 folds for each of 4332 candidates, totalling 12996 fits\n","Bestparamters: ({'criterion': 'entropy', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'random'})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G02O7SWoGqVo"},"outputs":[],"source":["tree_cv.best_params_#getting best parameters from cv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NtJLO8QRGqVo"},"outputs":[],"source":["tree_cv.best_score_#getting best score form cv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BsjmSDTHGqVp"},"outputs":[],"source":["dt1=DecisionTreeClassifier(criterion='entropy',max_depth=10,min_samples_leaf= 1,min_samples_split=3,splitter='random')#passing best parameter to decision tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QIpcP_d4GqVp"},"outputs":[],"source":["dt1.fit(X_train,y_train)#traing model with best parameter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hgxF9WufGqVp"},"outputs":[],"source":["y_hat1=dt1.predict(X_test)#predicting\n","y_hat1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FVl5l-psB6Oj"},"outputs":[],"source":["acc_test=accuracy_score(y_test,y_hat1)#checking accuracy\n","acc_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hj2ueoF_B6Oj"},"outputs":[],"source":["test_f1=f1_score(y_test,y_hat1)#f1_score\n","test_f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SNcxd2n4GqVp"},"outputs":[],"source":["print(classification_report(y_test,y_hat1))#it will give precision,recall,f1 scores and accuracy "]},{"cell_type":"markdown","metadata":{"id":"qZzYzUUNB6Ok"},"source":["# what is random forest\n","* A random forest is a supervised machine learning algorithm that is constructed from decision tree algorithms.\n","\n","\n","* A random forest is a machine learning technique that’s used to solve regression and classification problems. It utilizes       ensemble learning, which is a technique that combines many classifiers to provide solutions to complex problems.\n","\n","\n","* The (random forest) algorithm establishes the outcome based on the predictions of the decision trees. It predicts by taking the average or mean of the output from various trees. Increasing the number of trees increases the precision of the outcome.\n","\n","**working of Random forest**\n","![](rf.png)\n","\n","**Output side called as  Aggregation**\n","\n","\n","**What is bootstrap in random forest?**\n","* When training, each tree in a random forest learns from a random sample of the data points. The samples are drawn with replacement, known as bootstrapping, which means that some samples will be used multiple times in a single tree.\n","\n","\n","\n","\n","**For regression task it will take average**\n","\n","\n","\n","**For classification it will count the output** "]},{"cell_type":"markdown","metadata":{"id":"eOhvjBIxGqVp"},"source":["## RandomForest Implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CREynTLyGqVq"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier#importing randomforest\n","\n","rf_clf = RandomForestClassifier(n_estimators=100)#object creation ,taking 100 decision tree in random forest \n","rf_clf.fit(X_train,y_train)#training the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BCTj4_lVB6Ok"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"edJVbJxwB6Ok"},"outputs":[],"source":["RandomForestClassifier()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K6yeNrbLGqVq"},"outputs":[],"source":["y_predict=rf_clf.predict(X_test)#testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2PxP7zz7GqVq"},"outputs":[],"source":["\n","print(classification_report(y_test,y_predict))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q1693iiVGqVq"},"outputs":[],"source":["f_Score=f1_score(y_test,y_predict)\n","f_Score"]},{"cell_type":"markdown","metadata":{"id":"GGQzgT2GGqVq"},"source":["## Hyperparameter Tuning"]},{"cell_type":"markdown","metadata":{"id":"J2NDxMIKGqVq"},"source":["* n_estimators = number of trees in the foreset\n","\n","* max_features =These are the maximum number of features Random Forest is allowed to try in individual tree. There are multiple options available in Python to assign maximum features\n","\n","* max_depth =The depth of each tree in the forest. The deeper the tree, the more splits it has and it captures more information              about the data\n","\n","* min_samples_split =the minimum number of samples required to split an internal node. This can vary between considering at least one sample at each node to considering all of the samples at each node\n","\n","* min_samples_leaf = minimum number of data points allowed in a leaf node\n","* bootstrap = method for sampling data points (with or without replacement)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9xUOaCSsGqVr"},"outputs":[],"source":["#Random Search sets up a grid of hyperparameter values and selects random combinations to train the model and score.\n","#This allows you to explicitly control the number of parameter combinations that are attempted.\n","#The number of search iterations is set based on time or resources.\n","from sklearn.model_selection import RandomizedSearchCV\n","\n","n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]       #Number of decision trees\n","max_features = ['log2', 'sqrt']                                  #maximum number of features allowed to try in individual tree\n","max_depth = [int(x) for x in np.linspace(10, 110, num=11)]      #List Comprehension-using for loop in list\n","max_depth.append(None)\n","min_samples_split = [2, 5, 10]#minimum number of samples required to split an internal node\n","min_samples_leaf = [1, 2, 4]#minimum number of samples required to be at a leaf node.\n","bootstrap = [True, False]#sampling \n","\n","#dictionary for hyperparameters\n","random_grid = {'n_estimators': n_estimators, 'max_features': max_features,\n","               'max_depth': max_depth, 'min_samples_split': min_samples_split,\n","               'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}\n","\n","rf_clf1 = RandomForestClassifier(random_state=42)#model\n","\n","rf_cv = RandomizedSearchCV(estimator=rf_clf1, scoring='f1',param_distributions=random_grid, n_iter=100, cv=3, \n","                               verbose=2, random_state=42, n_jobs=-1)\n","\n","#estimator--number of decision tree\n","#scoring--->performance matrix to check performance\n","#param_distribution-->hyperparametes that we are going to provide \n","#n_iter--->Number of combinations to try\n","##cv------> number of folds\n","#verbose=Controls the verbosity:the greater the number, the more detail you will get.\n","#n_jobs----> if you specify n_jobs to -1, it will use all cores in CPU. If it is set to 1 or 2, it will use one or two cores only \n","\n","\n","\n","\n","\n","rf_cv.fit(X_train, y_train)                                  ##training data on randomsearch cv\n","rf_best_params = rf_cv.best_params_                          ##it will give you best parameters \n","print(f\"Best paramters: {rf_best_params})\")                  ##printing  best parameters\n"," \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HXDZsw6OGqVr"},"outputs":[],"source":["\n","#passing best parameter to randomforest\n","rf_clf2 = RandomForestClassifier(n_estimators= 1400, min_samples_split= 2, min_samples_leaf= 1, \n","                                 max_features= 'log2', max_depth= 40, bootstrap= False)\n","\n","\n","\n","rf_clf2.fit(X_train, y_train)\n","\n","y_predict=rf_clf2.predict(X_test)\n","\n","f1_score=f1_score(y_test,y_predict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e9vcwyZiGqVr"},"outputs":[],"source":["f1_score#calling variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2GoRwJjIB6Om"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pXr-ufsRB6Om"},"outputs":[],"source":[""]}],"metadata":{"colab":{"name":"Decision Tree RandomForest-checkpoint.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}