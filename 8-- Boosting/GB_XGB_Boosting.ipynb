{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZ7ARkFRFhdv"
   },
   "source": [
    "# What is GB_XGB ?\n",
    "* XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. A wide range of applications: Can be used to solve regression, classification\n",
    "\n",
    "\n",
    "\n",
    "* XGBoost is an implementation of gradient boosted decision trees designed for speed and performance\n",
    "\n",
    "\n",
    "\n",
    "* Boosting is an ensemble technique where new models are added to correct the errors made by existing models. Models are added sequentially until no further improvements can be made.\n",
    "\n",
    "\n",
    "\n",
    "* Gradient boosting is an approach where new models are created that predict the residuals or errors of prior models and then added together to make the final prediction. It is called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models.\n",
    "\n",
    "\n",
    "\n",
    "* This approach supports both regression and classification predictive modeling problems.\n",
    "\n",
    "\n",
    "## Decision tree,Bagging,Random forest,Boosting,Gradient Boosting,XGBoost\n",
    "![](xgbt.png)\n",
    "\n",
    "\n",
    "**Why does XGBoost perform so well?**\n",
    "* XGBoost and Gradient Boosting Machines  are both ensemble tree methods that apply the principle of boosting weak           learners using the gradient descent architecture. However, XGBoost improves upon the base GBM frameworkthrough systems optimization and algorithmic enhancements.\n",
    "\n",
    "\n",
    "#### 1.Regularization: \n",
    "* This is considered to be as a dominant factor of the algorithm. Regularization is a technique that is used to get rid of overfitting of the model. \n",
    "\n",
    "#### 2.Cross-Validation: \n",
    "* We use cross-validation by importing the function from sklearn but XGboost is enabled with inbuilt CV function.\n",
    "\n",
    "#### 3.Missing Value:  \n",
    "* It is designed in such a way that it can handle missing values. It finds out the trends in the missing values and apprehends them.\n",
    "\n",
    "#### 4.Flexibility:\n",
    "* It gives the support to objective functions. They are the function used to evaluate the performance of the model and also it can handle the user-defined validation metrics.\n",
    "\n",
    "\n",
    "\n",
    "## System Optimization\n",
    "\n",
    "#### Parallelization:\n",
    "* XGBoost approaches the process of sequential tree building using parallelized implementation. This is possible due to the interchangeable nature of loops used for building base learners; the outer loop that enumerates the leaf nodes of a tree, and the second inner loop that calculates the features. This nesting of loops limits parallelization because without completing the inner loop (more computationally demanding of the two), the outer loop cannot be started. Therefore, to improve run time, the order of loops is interchanged using initialization through a global scan of all instances and sorting using parallel threads. This switch improves algorithmic performance by offsetting any parallelization overheads in computation.\n",
    "\n",
    "#### Tree Pruning: \n",
    "* The stopping criterion for tree splitting within GBM framework is greedy in nature and depends on the negative loss criterion at the point of split. XGBoost uses ‘max_depth’ parameter as specified instead of criterion first, and starts pruning trees backward. This ‘depth-first’ approach improves computational performance significantly.\n",
    "\n",
    "\n",
    "#### Hardware Optimization:\n",
    "* This algorithm has been designed to make efficient use of hardware resources. This is accomplished by cache awareness by allocating internal buffers in each thread to store gradient statistics. Further enhancements such as ‘out-of-core’ computing optimize available disk space while handling big data-frames that do not fit into memory.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**When to Use XGBoost?**\n",
    "\n",
    "* 1> When you have large number of observations in training data.**\n",
    "\n",
    "* 2> Number features < number of observations in training data.**\n",
    "\n",
    "* 3> It performs well when data has mixture numerical and categorical features or just numeric features.**\n",
    "\n",
    "* 4> When the model performance metrics are to be considered.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Case: With the given features we need to predict the price of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1639371298000,
     "user": {
      "displayName": "Ajmal",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj7EIHjA82uo7mrD5GopTS-NhY0hu9EdKglQY-DZA=s64",
      "userId": "18063229917787663176"
     },
     "user_tz": -330
    },
    "id": "cn_WWqzXwONG"
   },
   "outputs": [],
   "source": [
    "## importing the libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lYNw766wONi"
   },
   "outputs": [],
   "source": [
    "##loading the dataset\n",
    "data=pd.read_csv('hp_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYHg_qx0Fhed"
   },
   "source": [
    "# Basic checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MM4-5vQJwONk",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()#first five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZP74kCMFhen"
   },
   "outputs": [],
   "source": [
    "data.tail()#last five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJVILPl7Fhep"
   },
   "outputs": [],
   "source": [
    "data.info()#to check null values and datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0w9wcHRYFhet"
   },
   "outputs": [],
   "source": [
    "data.describe()##used to view some basic statistical details like percentile, mean, std etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8749FNEnFhey"
   },
   "source": [
    "# Exploratory Data Analysis (EDA) --------------- TASK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wS4Mzf9SI6Pd"
   },
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gej4rQ5LFhfA"
   },
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWmzXNkqI6Pe"
   },
   "source": [
    "### Check the distribution for each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgmnvRLAFhfD"
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwjdLV-OFhfE"
   },
   "source": [
    "### Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2KUqnP_UFhfF"
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkDx7p_fFhfH"
   },
   "source": [
    "### Conversion of categorical columns into numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0oJSCw5FhfI"
   },
   "source": [
    "# 1.place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5_vulIqwONp"
   },
   "outputs": [],
   "source": [
    "## Data preprocessing\n",
    "## reducing the labels present in the place\n",
    "data.place.value_counts()#checking counts for each labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cRrG8QvwONr"
   },
   "outputs": [],
   "source": [
    "#area wise classification of place columns using loc function\n",
    "# for Phase 1\n",
    "\n",
    "\n",
    "# fetching the records that has place as Devarabeesana Halli and for those records, \n",
    "# we need to change the value for 'place' as Phase1\n",
    "data.loc[data['place']=='Devarabeesana Halli','place']='Phase1'\n",
    "\n",
    "# do the same for other places that belong to Phase1\n",
    "data.loc[data['place']=='KR Puram','place']='Phase1'\n",
    "data.loc[data['place']=='BTM Layout','place']='Phase1'\n",
    "data.loc[data['place']=='Abbaiah Reddy Layout','place']='Phase1'\n",
    "data.loc[data['place']=='Electronics City Phase 1','place']='Phase1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arFSxOwEwONt"
   },
   "outputs": [],
   "source": [
    "data.place.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khOUQiXzwONv"
   },
   "outputs": [],
   "source": [
    "#for phase 2\n",
    "data.loc[data['place']=='Ambalipura','place']='Phase2'\n",
    "data.loc[data['place']=='Yelahanka','place']='Phase2'\n",
    "data.loc[data['place']=='Whitefield','place']='Phase2'\n",
    "data.loc[data['place']=='Subramanyapura','place']='Phase2'\n",
    "data.loc[data['place']=='Yelachenahalli','place']='Phase2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yba5nkKKwONw"
   },
   "outputs": [],
   "source": [
    "#for phase 3\n",
    "data.loc[data['place']=='Sarakki Nagar','place']='Phase3'\n",
    "data.loc[data['place']=='Malleshwaram','place']='Phase3'\n",
    "data.loc[data['place']=='Gunjur','place']='Phase3'\n",
    "data.loc[data['place']=='Frazer Town','place']='Phase3'\n",
    "data.loc[data['place']=='Rajaji Nagar','place']='Phase3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzFtoGDFwONy"
   },
   "outputs": [],
   "source": [
    "data.place.value_counts() #finally  15 labels classified into 3 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxdg_MUlwON0"
   },
   "outputs": [],
   "source": [
    "## conversion into numerical \n",
    "# ON The basis of count \n",
    "data.loc[data['place']=='Phase1','place']=1\n",
    "data.loc[data['place']=='Phase2','place']=2\n",
    "data.loc[data['place']=='Phase3','place']=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yaK8R1lRwON1"
   },
   "outputs": [],
   "source": [
    "data.place.value_counts()#checking for conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FU5AlEInFhfX"
   },
   "source": [
    "# 2.built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XG7WXl7WwON3"
   },
   "outputs": [],
   "source": [
    "## preprocessing built\n",
    "#checking count for each label\n",
    "data.built.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CS8qkjIVwON5"
   },
   "outputs": [],
   "source": [
    "#coversion on the basis of counts \n",
    "# whichever label has higher counts give it higher weightage\n",
    "data.loc[data['built']=='Super built-up  Area','built']=1\n",
    "data.loc[data['built']=='Built-up  Area','built']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-xa3z7OwON7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.built.value_counts()#checking count for each label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DmznO7RFhfa"
   },
   "source": [
    "# 3. sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etmS12n-wON8"
   },
   "outputs": [],
   "source": [
    "data.sale.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gk9KmULtFhfc"
   },
   "outputs": [],
   "source": [
    "data.drop(['sale'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYPOBH-CI6Pj"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEHzf6scI6Pk"
   },
   "outputs": [],
   "source": [
    "data['place'] = data['place'].astype('int64')\n",
    "data['built'] = data['built'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZjQwkccI6Pk"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUHAN1CjFhfl"
   },
   "source": [
    "# checking for outlier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwuO17CgFhfm"
   },
   "outputs": [],
   "source": [
    "# let's see how data is distributed for every column\n",
    "plt.figure(figsize=(20,25), facecolor='white')\n",
    "plotnumber = 1 #counter\n",
    "\n",
    "for column in data:#columns form data1\n",
    "    if plotnumber<=9 :#checking whether plot number is less or equal to 6\n",
    "        ax = plt.subplot(4,2,plotnumber)# \n",
    "        sns.boxplot(data[column])# ploting boxplot for outlier \n",
    "        plt.xlabel(column,fontsize=20)\n",
    "        \n",
    "    plotnumber+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8fq6eK1Fhfn"
   },
   "source": [
    "# Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rR75aeDLFhfn"
   },
   "outputs": [],
   "source": [
    "## Checking correlation\n",
    "\n",
    "plt.figure(figsize=(10,10))#canvas size\n",
    "sns.heatmap(data.corr(), annot=True, cmap=\"RdYlGn\", annot_kws={\"size\":15})#plotting heat map to check correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgguasHmFhfo"
   },
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Xpo8TCKwON-"
   },
   "outputs": [],
   "source": [
    "## creating X and y\n",
    "X=data.loc[:,['place', 'built', 'sqft','yearsOld', 'floor', 'totalFloor',            #independent variable \n",
    "       'bhk']]\n",
    "y=data.price#dependent variable or target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCls7uWgwON_"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWj-_FtvwOOB"
   },
   "outputs": [],
   "source": [
    "## creating training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mn1YdXEQFhfv"
   },
   "source": [
    "# what is Gradient Boosting ?\n",
    "* Gradient boosting is a type of machine learning boosting. It relies on the intuition that when combined with previous models, the best possible next model,  minimizes the overall prediction error. The key idea is to set the target outcomes for this next model in order to minimize the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4urIXXelwOOD"
   },
   "outputs": [],
   "source": [
    "## importing the model library\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbm=GradientBoostingRegressor() ## object creation\n",
    "gbm.fit(X_train,y_train) ## fitting the data\n",
    "y_gbm=gbm.predict(X_test)#predicting the price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GK_yuUfBwOOE"
   },
   "outputs": [],
   "source": [
    "## evaluatin the model\n",
    "from sklearn.metrics import r2_score# to check model performance\n",
    "r2_score(y_test,y_gbm)#checking r2score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjvBpiuzwOOH"
   },
   "outputs": [],
   "source": [
    "adj_r2_score=1-(1-0.99)*(1050-1)/(1050-7-1)#adjusted r2 score\n",
    "adj_r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQOtrZXbwOOJ"
   },
   "outputs": [],
   "source": [
    "X_test.shape#rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U42bzzuwwOOL"
   },
   "outputs": [],
   "source": [
    "## Installing XGB library\n",
    "\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqFagaq5Wz2N"
   },
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlIzGzhBwOOP"
   },
   "outputs": [],
   "source": [
    "## model creation\n",
    "#importing the model library\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#xgb_r = XGBRegressor(objective ='reg:linear', n_estimators = 10, seed = 123)\n",
    "\n",
    "\n",
    "xgb_r= XGBRegressor() ## object creation\n",
    "xgb_r.fit(X_train,y_train)# fitting the data\n",
    "y_hat=xgb_r.predict(X_test)#predicting the price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJJ5BdfHwOOQ"
   },
   "outputs": [],
   "source": [
    "r2_score(y_test,y_hat)#R2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D0V2oh3FhgA"
   },
   "source": [
    "# Hyper parameter tunning in XG bost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1_0yE5zFhgB"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]#List Comprehension-using for loop in list\n",
    "max_depth = [3,4,5,6]#The maximum depth of a tree\n",
    "learning_rate=[0.1,0.2,0.3] #Typical final values to be used: 0.01-0.2\n",
    "gamma=[0, 1, 2, 3, 4] # Gamma specifies the minimum loss reduction required to make a split. It controls the overfitting. Ranges from 0 to ∞.\n",
    "subsample=[0.5,0.7,1]#Lower values make the algorithm more conservative and prevents overfitting but too \n",
    "                     #small values might lead to under-fitting.Typical values: 0.5-1. Range: (0,1)\n",
    "colsample_bytree=[0.5,0.7,1]#Denotes the fraction of columns to be randomly samples for each tree. Ranges from 0 to 1\n",
    "\n",
    "params={\n",
    "    'max_depth':max_depth,'learning_rate':learning_rate,'n_estimators':n_estimators,\n",
    "     'gamma':gamma, 'subsample':subsample, 'colsample_bytree':colsample_bytree\n",
    "}\n",
    "\n",
    "XGB=XGBRegressor(random_state=42)\n",
    "\n",
    "rcv= RandomizedSearchCV(XGB, scoring='r2',param_distributions=params, n_iter=100, cv=3, \n",
    "                                random_state=42, n_jobs=-1)\n",
    "                              \n",
    "#estimator--number of decision tree\n",
    "#scoring--->performance matrix to check performance\n",
    "#param_distribution-->hyperparametes(dictionary we created)\n",
    "#n_iter--->Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution.default=10\n",
    "##cv------> number of folds\n",
    "#verbose=Controls the verbosity: the higher, the more messages.\n",
    "#n_jobs---->Number of jobs to run in parallel,-1 means using all processors.\n",
    "                        \n",
    "rcv.fit(X_train, y_train) ##training data on randomsearch cv.\n",
    "cv_best_params = rcv.best_params_ ##it will give you best parameters \n",
    "print(f\"Best paramters: {cv_best_params}\")  ##printing  best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_saEpmr3I6Po"
   },
   "outputs": [],
   "source": [
    "cv_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwprxhXkI6Po"
   },
   "outputs": [],
   "source": [
    "XGB2=XGBRegressor(subsample= 0.5,\n",
    " n_estimators= 2000,\n",
    " max_depth= 3,\n",
    " learning_rate= 0.3, gamma=2,\n",
    " colsample_bytree= 0.5)\n",
    "\n",
    "XGB2.fit(X_train, y_train)#training \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=XGB2.predict(X_test)#testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score2=r2_score(y_test,y_predict)#checking performance\n",
    "r2_score2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFKRQfmwXKBb"
   },
   "source": [
    "## XGBoost\n",
    "### Pros\n",
    "1. Less feature engineering required (No need for scaling, normalizing data, can also handle missing values well)\n",
    "2. Feature importance can be found out(it output importance of each feature, can be used for feature selection)\n",
    "3. Fast to interpret\n",
    "4. Outliers have minimal impact.\n",
    "5. Handles large sized datasets well.\n",
    "6. Good Execution speed\n",
    "7. Good model performance (wins most of the Kaggle competitions)\n",
    "8. Less prone to overfitting\n",
    "\n",
    "### Cons\n",
    "1. Difficult interpretation , visualization tough\n",
    "2. Overfitting possible if parameters not tuned proper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zb6KOh4-XQ4m"
   },
   "source": [
    "![](def2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GB_XGB_Boosting.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
